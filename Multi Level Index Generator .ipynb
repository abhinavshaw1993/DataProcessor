{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owd:  F:\\Projects\\DataProcessor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "owd = \"F:\\Projects\\DataProcessor\"\n",
    "print(\"owd: \", owd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sms_details_Unnamed: 3_min', 'sms_details_Unnamed: 3_max', 'sms_details_Unnamed: 3_mean', 'sms_details_Unnamed: 3_median', 'sms_details_Unnamed: 3_sum', 'sms_details_time_delta', 'sms_details_count']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(owd)\n",
    "data_dir = owd+\"\\StudentLife Data\"\n",
    "os.chdir(data_dir)\n",
    "dir_list = [x for x in os.listdir('.')]\n",
    "dir_list = dir_list[1:2]\n",
    "converter_dict = {'time':pd.to_datetime}\n",
    "\n",
    "train_set = pd.DataFrame\n",
    "test_set = pd.DataFrame\n",
    "\n",
    "for folder in dir_list:\n",
    "    path = data_dir + \"\\\\\"+ folder\n",
    "    os.chdir(path) \n",
    "    file_list = [file[:-12]  for file in os.listdir() if \"train_x\" in file]\n",
    "#     file_list = file_list[5:8]\n",
    "    feature_test_list = []\n",
    "    feature_train_list = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        \n",
    "        raw_feature_train = pd.read_csv(file+ \"_train_x.csv\",\n",
    "                                          skip_blank_lines=False,\n",
    "                                          index_col=0,\n",
    "                                          converters=converter_dict)\n",
    "\n",
    "        raw_feature_train_y_indices = pd.read_csv(file+ \"_train_y_indices.csv\", \n",
    "                                                  skip_blank_lines=False, \n",
    "                                                  index_col=0)\n",
    "\n",
    "        # to bring the values from 0-4.\n",
    "        raw_feature_train[\"stress_level\"] += -1\n",
    "\n",
    "        # Hardcore indexing, to convert single index to multi so that max, min and avg can be taken easily.\n",
    "        # Train set.\n",
    "        list_a = []\n",
    "        list_b = raw_feature_train.index.values\n",
    "\n",
    "        indices = list(raw_feature_train_y_indices.indices.values)\n",
    "        indices = [-1] + indices\n",
    "\n",
    "        for indx in range(1, len(indices)) :\n",
    "            list_a += [indices[indx] for i in range(indices[indx]-indices[indx-1])]\n",
    "\n",
    "        index_keys = [\n",
    "            np.array(list_a),\n",
    "            np.array(list_b)        \n",
    "        ]\n",
    "\n",
    "        raw_feature_train.set_index(keys=index_keys, inplace=True)\n",
    "\n",
    "        feature_list = list(raw_feature_train.columns.values) \n",
    "        feature_list.remove(\"student_id\")\n",
    "        feature_list.remove(\"time\")\n",
    "        feature_list.remove(\"stress_level\")\n",
    "        \n",
    "        # Colapsing Multindex to fin min, max and mean of the seq. \n",
    "        raw_feature_train_min = raw_feature_train.min(level=0)\n",
    "        raw_feature_train_max = raw_feature_train.max(level=0)\n",
    "        \n",
    "\n",
    "        # Mean, Median and Count also creating the column list.\n",
    "        raw_feature_train_mean = raw_feature_train.mean(level=0)\n",
    "        raw_feature_train_median = raw_feature_train.median(level=0)\n",
    "        raw_feature_train_sum = raw_feature_train.sum(level=0)\n",
    "        raw_feature_train_count = raw_feature_train.count(level=0)\n",
    "        \n",
    "        # Explicit delta given to the model. Since all the counts are same, we just need to pick any one column.\n",
    "        raw_feature_train_count = raw_feature_train_count.iloc[:,0]\n",
    "\n",
    "        #getting time delta in secconds\n",
    "        time_delta = raw_feature_train_max[\"time\"] - raw_feature_train_min[\"time\"] \n",
    "        time_delta = time_delta.apply(timedelta.total_seconds)\n",
    "        \n",
    "        \n",
    "        raw_feature_train = pd.concat([  raw_feature_train_min.loc[:,feature_list], \n",
    "                                         raw_feature_train_max.loc[:, feature_list],\n",
    "                                         raw_feature_train_mean.loc[:, feature_list],\n",
    "                                         raw_feature_train_median.loc[:, feature_list],\n",
    "                                         raw_feature_train_sum.loc[:, feature_list],\n",
    "                                         time_delta,\n",
    "                                         raw_feature_train_count], \n",
    "                                         axis=1,\n",
    "                                         ignore_index=True)\n",
    "        \n",
    "        # preparing column list and renaming the columns.\n",
    "        column_list = [file + \"_\"+feature + \"_min\" for feature in feature_list]\n",
    "        column_list = column_list + [file + \"_\"+feature + \"_max\" for feature in feature_list]\n",
    "        column_list = column_list + [file + \"_\"+feature + \"_mean\" for feature in feature_list]\n",
    "        column_list = column_list + [file + \"_\"+feature + \"_median\" for feature in feature_list]\n",
    "        column_list = column_list + [file + \"_\"+feature + \"_sum\" for feature in feature_list]\n",
    "        column_list = column_list + [file + \"_time_delta\"]\n",
    "        column_list = column_list + [file + \"_count\"]\n",
    "        raw_feature_train.columns = column_list\n",
    "        \n",
    "        if \"sms\" in file:\n",
    "            print (column_list)\n",
    "        \n",
    "        # Reseeting Index\n",
    "        raw_feature_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        feature_train_list.append(raw_feature_train)\n",
    "    \n",
    "    \n",
    "    #splitting stress labels. Need to do this only once, since stress level will be the same for all features.\n",
    "    stress_levels = raw_feature_train_min[\"stress_level\"]\n",
    "    stress_levels.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # grabbing the student_id.\n",
    "    student_id = raw_feature_train_min[\"student_id\"]\n",
    "    student_id.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # concatenating every thing for final dataset.\n",
    "    concatenated_train = pd.concat([student_id]+feature_train_list+[stress_levels], axis=1)\n",
    "    \n",
    "    var_exists = 'train_set' in locals() or 'train_set' in globals()\n",
    "    \n",
    "    if False:\n",
    "        train_set = train_set.append(other=concatenated_train, ignore_index=True)\n",
    "    else:\n",
    "        train_set = pd.DataFrame(concatenated_train)\n",
    "\n",
    "os.chdir(data_dir+'/VarableLength Aggregates')\n",
    "train_set.to_csv(\"variable_interval_aggregate_train.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
